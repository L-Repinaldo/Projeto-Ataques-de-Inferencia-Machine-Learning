# Projeto-Ataques-de-Inferencia-Machine-Learning


ğŸ§¬ Resumo do Projeto Machine Learning â€” AnÃ¡lise de Vazamento de Dados com Ataques de InferÃªncia em ML e Privacidade Diferencial

O Projeto B Ã© um ambiente de pesquisa experimental voltado a investigar como modelos de aprendizado de mÃ¡quina podem vazar informaÃ§Ãµes sensÃ­veis quando treinados sobre dados protegidos por Privacidade Diferencial (DP).
Ele utiliza o mesmo banco de dados do Projeto A (Sistema de RH), porÃ©m com foco exclusivo em analisar riscos, simular ataques e medir a eficÃ¡cia da proteÃ§Ã£o.

ğŸ¯ Objetivo Geral

Avaliar, de forma prÃ¡tica e reproduzÃ­vel:

Quais tipos de ataques de inferÃªncia conseguem vazar informaÃ§Ãµes sensÃ­veis.

Como nÃ­veis diferentes de ruÃ­do (Îµ e Î´) impactam a probabilidade de vazamento.

O trade-off entre privacidade e acurÃ¡cia dos modelos.

A eficiÃªncia de mecanismos como Laplace, Gaussian e DP-SGD.

Quais atributos e padrÃµes sÃ£o mais suscetÃ­veis a serem inferidos.

O projeto culmina em um artigo acadÃªmico comparando ataques, defesas e resultados.

ğŸ—ï¸ Base de Dados

O Projeto B acessa uma cÃ³pia ou segmento controlado do banco do Projeto A, incluindo:

funcionÃ¡rios (setor, faixa salarial, idade, cargo)

avaliaÃ§Ãµes periÃ³dicas

benefÃ­cios utilizados

estrutura de setores e gerentes

Esses dados sÃ£o ricos, sensÃ­veis e ideais para simular cenÃ¡rios reais de vazamento.

ğŸ§  Tipos de Ataques Implementados
1. Membership Inference Attack

Determina se um funcionÃ¡rio especÃ­fico fez parte do conjunto de treinamento do modelo.

2. Attribute Inference Attack

Tenta prever atributos sensÃ­veis ocultos, como:

faixa salarial

uso de determinados benefÃ­cios

nota de avaliaÃ§Ã£o

setor de atuaÃ§Ã£o

3. Model Inversion Attack

ReconstrÃ³i caracterÃ­sticas aproximadas do indivÃ­duo com base nas saÃ­das do modelo.

Esses ataques sÃ£o comparados com diferentes nÃ­veis de DP.

ğŸ”’ Mecanismos de Privacidade Avaliados

O Projeto B testa e compara:

Laplace Mechanism (para consultas agregadas)

Gaussian Mechanism

DP-SGD (treinamento com privacidade diferencial)

PerturbaÃ§Ã£o de labels e features

Query-level vs. model-level DP

Cada mecanismo Ã© analisado quanto a:

proteÃ§Ã£o efetiva

impacto na acurÃ¡cia

resistÃªncia aos ataques

tempo de treinamento

ğŸ“Š MÃ©tricas e Resultados

O sistema produz:

grÃ¡ficos de vazamento por Îµ

curvas de ataque vs. defesa

impacto de DP na acurÃ¡cia do modelo

estimativas de risco individual por atributo

tabelas comparativas entre mecanismos

Esses resultados formam a base do artigo.

ğŸ”¬ Metodologia

Importar dados do Projeto A (cÃ³pia sanitizada).

Separar features sensÃ­veis e nÃ£o sensÃ­veis.

Treinar modelos com e sem DP (ex.: regressÃ£o, random forests, redes simples).

Aplicar ataques de inferÃªncia.

Medir taxa de sucesso.

Analisar o comportamento sob diferentes valores de Îµ.

Gerar grÃ¡ficos, relatÃ³rios e conclusÃµes.

ğŸ“Œ RelaÃ§Ã£o com o Projeto A

O Projeto A Ã© o sistema â€œreal protegidoâ€.

O Projeto B Ã© o ambiente de experimentaÃ§Ã£o que tenta quebrar ou inferir informaÃ§Ãµes do mesmo banco.

A comparaÃ§Ã£o entre ruÃ­do aplicado no A e ataques no B permite gerar um artigo forte e bem fundamentado.
